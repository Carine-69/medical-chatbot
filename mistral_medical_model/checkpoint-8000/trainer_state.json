{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.942423796200854,
  "eval_steps": 500,
  "global_step": 8000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0058901487262553376,
      "grad_norm": 1.2941187620162964,
      "learning_rate": 4.971139121215691e-05,
      "loss": 1.7403553771972655,
      "step": 50
    },
    {
      "epoch": 0.011780297452510675,
      "grad_norm": 1.6753960847854614,
      "learning_rate": 4.9416892449051714e-05,
      "loss": 0.7239531707763672,
      "step": 100
    },
    {
      "epoch": 0.017670446178766015,
      "grad_norm": 1.747673511505127,
      "learning_rate": 4.912239368594652e-05,
      "loss": 0.6897053527832031,
      "step": 150
    },
    {
      "epoch": 0.02356059490502135,
      "grad_norm": 1.8251997232437134,
      "learning_rate": 4.882789492284133e-05,
      "loss": 0.6560421752929687,
      "step": 200
    },
    {
      "epoch": 0.02945074363127669,
      "grad_norm": 1.6400120258331299,
      "learning_rate": 4.853339615973613e-05,
      "loss": 0.6693180847167969,
      "step": 250
    },
    {
      "epoch": 0.03534089235753203,
      "grad_norm": 1.611698031425476,
      "learning_rate": 4.823889739663094e-05,
      "loss": 0.660993423461914,
      "step": 300
    },
    {
      "epoch": 0.04123104108378737,
      "grad_norm": 1.992181420326233,
      "learning_rate": 4.794439863352574e-05,
      "loss": 0.6611607360839844,
      "step": 350
    },
    {
      "epoch": 0.0471211898100427,
      "grad_norm": 1.6070436239242554,
      "learning_rate": 4.764989987042055e-05,
      "loss": 0.6416822814941406,
      "step": 400
    },
    {
      "epoch": 0.05301133853629804,
      "grad_norm": 1.6938332319259644,
      "learning_rate": 4.735540110731535e-05,
      "loss": 0.6376131057739258,
      "step": 450
    },
    {
      "epoch": 0.05890148726255338,
      "grad_norm": 1.5767772197723389,
      "learning_rate": 4.7060902344210155e-05,
      "loss": 0.6137507629394531,
      "step": 500
    },
    {
      "epoch": 0.06479163598880872,
      "grad_norm": 1.7263660430908203,
      "learning_rate": 4.676640358110496e-05,
      "loss": 0.6317708206176758,
      "step": 550
    },
    {
      "epoch": 0.07068178471506406,
      "grad_norm": 2.165389060974121,
      "learning_rate": 4.647190481799977e-05,
      "loss": 0.63582763671875,
      "step": 600
    },
    {
      "epoch": 0.07657193344131939,
      "grad_norm": 1.2909202575683594,
      "learning_rate": 4.617740605489457e-05,
      "loss": 0.610918083190918,
      "step": 650
    },
    {
      "epoch": 0.08246208216757474,
      "grad_norm": 2.04936146736145,
      "learning_rate": 4.588290729178938e-05,
      "loss": 0.5993098831176757,
      "step": 700
    },
    {
      "epoch": 0.08835223089383007,
      "grad_norm": 1.8733797073364258,
      "learning_rate": 4.558840852868419e-05,
      "loss": 0.6125025939941406,
      "step": 750
    },
    {
      "epoch": 0.0942423796200854,
      "grad_norm": 1.582804799079895,
      "learning_rate": 4.529390976557899e-05,
      "loss": 0.5584768676757812,
      "step": 800
    },
    {
      "epoch": 0.10013252834634075,
      "grad_norm": 1.9566656351089478,
      "learning_rate": 4.499941100247379e-05,
      "loss": 0.567276725769043,
      "step": 850
    },
    {
      "epoch": 0.10602267707259608,
      "grad_norm": 1.9025871753692627,
      "learning_rate": 4.4704912239368596e-05,
      "loss": 0.5649621200561523,
      "step": 900
    },
    {
      "epoch": 0.11191282579885142,
      "grad_norm": 1.8953393697738647,
      "learning_rate": 4.4410413476263404e-05,
      "loss": 0.5729150390625,
      "step": 950
    },
    {
      "epoch": 0.11780297452510675,
      "grad_norm": 1.7023439407348633,
      "learning_rate": 4.4115914713158205e-05,
      "loss": 0.5878829574584961,
      "step": 1000
    },
    {
      "epoch": 0.1236931232513621,
      "grad_norm": 2.4896883964538574,
      "learning_rate": 4.382141595005301e-05,
      "loss": 0.5806378555297852,
      "step": 1050
    },
    {
      "epoch": 0.12958327197761743,
      "grad_norm": 2.4915859699249268,
      "learning_rate": 4.352691718694782e-05,
      "loss": 0.5652131271362305,
      "step": 1100
    },
    {
      "epoch": 0.13547342070387278,
      "grad_norm": 2.200421094894409,
      "learning_rate": 4.323241842384263e-05,
      "loss": 0.5868469619750977,
      "step": 1150
    },
    {
      "epoch": 0.14136356943012812,
      "grad_norm": 1.813688039779663,
      "learning_rate": 4.293791966073742e-05,
      "loss": 0.5833338165283203,
      "step": 1200
    },
    {
      "epoch": 0.14725371815638344,
      "grad_norm": 1.7450164556503296,
      "learning_rate": 4.264342089763223e-05,
      "loss": 0.5578608322143555,
      "step": 1250
    },
    {
      "epoch": 0.15314386688263878,
      "grad_norm": 1.4213824272155762,
      "learning_rate": 4.234892213452704e-05,
      "loss": 0.579702262878418,
      "step": 1300
    },
    {
      "epoch": 0.15903401560889413,
      "grad_norm": 1.1198606491088867,
      "learning_rate": 4.2054423371421845e-05,
      "loss": 0.5357888793945312,
      "step": 1350
    },
    {
      "epoch": 0.16492416433514948,
      "grad_norm": 2.036064386367798,
      "learning_rate": 4.1759924608316646e-05,
      "loss": 0.5583599853515625,
      "step": 1400
    },
    {
      "epoch": 0.1708143130614048,
      "grad_norm": 1.808705449104309,
      "learning_rate": 4.146542584521145e-05,
      "loss": 0.5436961364746093,
      "step": 1450
    },
    {
      "epoch": 0.17670446178766014,
      "grad_norm": 2.290233612060547,
      "learning_rate": 4.117092708210626e-05,
      "loss": 0.5331709671020508,
      "step": 1500
    },
    {
      "epoch": 0.18259461051391548,
      "grad_norm": 1.6860301494598389,
      "learning_rate": 4.087642831900106e-05,
      "loss": 0.5474380111694336,
      "step": 1550
    },
    {
      "epoch": 0.1884847592401708,
      "grad_norm": 1.6764612197875977,
      "learning_rate": 4.058192955589586e-05,
      "loss": 0.5482875061035156,
      "step": 1600
    },
    {
      "epoch": 0.19437490796642615,
      "grad_norm": 2.026489496231079,
      "learning_rate": 4.028743079279067e-05,
      "loss": 0.5686441802978516,
      "step": 1650
    },
    {
      "epoch": 0.2002650566926815,
      "grad_norm": 1.8851786851882935,
      "learning_rate": 3.999293202968548e-05,
      "loss": 0.559119987487793,
      "step": 1700
    },
    {
      "epoch": 0.20615520541893684,
      "grad_norm": 1.5725171566009521,
      "learning_rate": 3.969843326658028e-05,
      "loss": 0.5612183380126953,
      "step": 1750
    },
    {
      "epoch": 0.21204535414519216,
      "grad_norm": 2.031541585922241,
      "learning_rate": 3.9403934503475086e-05,
      "loss": 0.561434555053711,
      "step": 1800
    },
    {
      "epoch": 0.2179355028714475,
      "grad_norm": 1.6266528367996216,
      "learning_rate": 3.9109435740369894e-05,
      "loss": 0.5346672058105468,
      "step": 1850
    },
    {
      "epoch": 0.22382565159770285,
      "grad_norm": 1.876684308052063,
      "learning_rate": 3.88149369772647e-05,
      "loss": 0.5655668258666993,
      "step": 1900
    },
    {
      "epoch": 0.2297158003239582,
      "grad_norm": 1.6678053140640259,
      "learning_rate": 3.85204382141595e-05,
      "loss": 0.5720650482177735,
      "step": 1950
    },
    {
      "epoch": 0.2356059490502135,
      "grad_norm": 1.3960093259811401,
      "learning_rate": 3.822593945105431e-05,
      "loss": 0.5517573928833008,
      "step": 2000
    },
    {
      "epoch": 0.24149609777646885,
      "grad_norm": 0.395351767539978,
      "learning_rate": 3.793144068794911e-05,
      "loss": 0.538932991027832,
      "step": 2050
    },
    {
      "epoch": 0.2473862465027242,
      "grad_norm": 0.3818427324295044,
      "learning_rate": 3.763694192484392e-05,
      "loss": 0.5538714599609375,
      "step": 2100
    },
    {
      "epoch": 0.25327639522897955,
      "grad_norm": 0.5548568367958069,
      "learning_rate": 3.734244316173872e-05,
      "loss": 0.5717246627807617,
      "step": 2150
    },
    {
      "epoch": 0.25916654395523486,
      "grad_norm": 0.4408022165298462,
      "learning_rate": 3.704794439863353e-05,
      "loss": 0.5592363357543946,
      "step": 2200
    },
    {
      "epoch": 0.2650566926814902,
      "grad_norm": 0.4598452150821686,
      "learning_rate": 3.6753445635528335e-05,
      "loss": 0.5655218505859375,
      "step": 2250
    },
    {
      "epoch": 0.27094684140774555,
      "grad_norm": 0.404998242855072,
      "learning_rate": 3.6458946872423136e-05,
      "loss": 0.5392188644409179,
      "step": 2300
    },
    {
      "epoch": 0.27683699013400087,
      "grad_norm": 0.38343799114227295,
      "learning_rate": 3.6164448109317944e-05,
      "loss": 0.5430197906494141,
      "step": 2350
    },
    {
      "epoch": 0.28272713886025624,
      "grad_norm": 0.4621484577655792,
      "learning_rate": 3.586994934621275e-05,
      "loss": 0.5504119110107422,
      "step": 2400
    },
    {
      "epoch": 0.28861728758651156,
      "grad_norm": 0.44721800088882446,
      "learning_rate": 3.557545058310755e-05,
      "loss": 0.552396583557129,
      "step": 2450
    },
    {
      "epoch": 0.2945074363127669,
      "grad_norm": 0.4717475175857544,
      "learning_rate": 3.528095182000235e-05,
      "loss": 0.5528832626342773,
      "step": 2500
    },
    {
      "epoch": 0.30039758503902225,
      "grad_norm": 0.47832250595092773,
      "learning_rate": 3.498645305689716e-05,
      "loss": 0.5475437545776367,
      "step": 2550
    },
    {
      "epoch": 0.30628773376527757,
      "grad_norm": 0.5509440898895264,
      "learning_rate": 3.469195429379197e-05,
      "loss": 0.5418649673461914,
      "step": 2600
    },
    {
      "epoch": 0.3121778824915329,
      "grad_norm": 0.43222007155418396,
      "learning_rate": 3.4397455530686776e-05,
      "loss": 0.5564675140380859,
      "step": 2650
    },
    {
      "epoch": 0.31806803121778826,
      "grad_norm": 0.4327380955219269,
      "learning_rate": 3.410295676758158e-05,
      "loss": 0.5251589965820312,
      "step": 2700
    },
    {
      "epoch": 0.3239581799440436,
      "grad_norm": 0.442979633808136,
      "learning_rate": 3.3808458004476385e-05,
      "loss": 0.5658828353881836,
      "step": 2750
    },
    {
      "epoch": 0.32984832867029895,
      "grad_norm": 0.38453423976898193,
      "learning_rate": 3.351395924137119e-05,
      "loss": 0.5546068954467773,
      "step": 2800
    },
    {
      "epoch": 0.33573847739655427,
      "grad_norm": 0.5553479194641113,
      "learning_rate": 3.321946047826599e-05,
      "loss": 0.603092041015625,
      "step": 2850
    },
    {
      "epoch": 0.3416286261228096,
      "grad_norm": 0.5483304262161255,
      "learning_rate": 3.2924961715160794e-05,
      "loss": 0.5430016326904297,
      "step": 2900
    },
    {
      "epoch": 0.34751877484906496,
      "grad_norm": 0.3833921253681183,
      "learning_rate": 3.26304629520556e-05,
      "loss": 0.552933235168457,
      "step": 2950
    },
    {
      "epoch": 0.3534089235753203,
      "grad_norm": 0.43580007553100586,
      "learning_rate": 3.233596418895041e-05,
      "loss": 0.5476341247558594,
      "step": 3000
    },
    {
      "epoch": 0.3592990723015756,
      "grad_norm": 0.5573161840438843,
      "learning_rate": 3.204146542584521e-05,
      "loss": 0.5698468399047851,
      "step": 3050
    },
    {
      "epoch": 0.36518922102783097,
      "grad_norm": 0.4736177623271942,
      "learning_rate": 3.174696666274002e-05,
      "loss": 0.5591313171386719,
      "step": 3100
    },
    {
      "epoch": 0.3710793697540863,
      "grad_norm": 0.4501194357872009,
      "learning_rate": 3.1452467899634825e-05,
      "loss": 0.5632185363769531,
      "step": 3150
    },
    {
      "epoch": 0.3769695184803416,
      "grad_norm": 0.3763764500617981,
      "learning_rate": 3.115796913652963e-05,
      "loss": 0.5444170761108399,
      "step": 3200
    },
    {
      "epoch": 0.382859667206597,
      "grad_norm": 0.588134229183197,
      "learning_rate": 3.0863470373424434e-05,
      "loss": 0.5429048538208008,
      "step": 3250
    },
    {
      "epoch": 0.3887498159328523,
      "grad_norm": 0.394212931394577,
      "learning_rate": 3.0568971610319235e-05,
      "loss": 0.5491457748413086,
      "step": 3300
    },
    {
      "epoch": 0.39463996465910767,
      "grad_norm": 0.42958351969718933,
      "learning_rate": 3.0274472847214043e-05,
      "loss": 0.5538850402832032,
      "step": 3350
    },
    {
      "epoch": 0.400530113385363,
      "grad_norm": 0.4349222779273987,
      "learning_rate": 2.9979974084108847e-05,
      "loss": 0.5667119216918945,
      "step": 3400
    },
    {
      "epoch": 0.4064202621116183,
      "grad_norm": 0.47463831305503845,
      "learning_rate": 2.9685475321003654e-05,
      "loss": 0.5373799896240234,
      "step": 3450
    },
    {
      "epoch": 0.4123104108378737,
      "grad_norm": 0.49269214272499084,
      "learning_rate": 2.939097655789846e-05,
      "loss": 0.5329120254516602,
      "step": 3500
    },
    {
      "epoch": 0.418200559564129,
      "grad_norm": 0.54723060131073,
      "learning_rate": 2.9096477794793263e-05,
      "loss": 0.5363784790039062,
      "step": 3550
    },
    {
      "epoch": 0.4240907082903843,
      "grad_norm": 0.5561466813087463,
      "learning_rate": 2.880197903168807e-05,
      "loss": 0.5281709289550781,
      "step": 3600
    },
    {
      "epoch": 0.4299808570166397,
      "grad_norm": 0.38996806740760803,
      "learning_rate": 2.8507480268582875e-05,
      "loss": 0.5627598190307617,
      "step": 3650
    },
    {
      "epoch": 0.435871005742895,
      "grad_norm": 0.372369647026062,
      "learning_rate": 2.8212981505477676e-05,
      "loss": 0.5432020568847656,
      "step": 3700
    },
    {
      "epoch": 0.4417611544691503,
      "grad_norm": 0.28432905673980713,
      "learning_rate": 2.791848274237248e-05,
      "loss": 0.5130749130249024,
      "step": 3750
    },
    {
      "epoch": 0.4476513031954057,
      "grad_norm": 0.41390323638916016,
      "learning_rate": 2.7623983979267288e-05,
      "loss": 0.542567024230957,
      "step": 3800
    },
    {
      "epoch": 0.453541451921661,
      "grad_norm": 0.4123648703098297,
      "learning_rate": 2.7329485216162092e-05,
      "loss": 0.553874626159668,
      "step": 3850
    },
    {
      "epoch": 0.4594316006479164,
      "grad_norm": 0.4141990542411804,
      "learning_rate": 2.70349864530569e-05,
      "loss": 0.5431841659545898,
      "step": 3900
    },
    {
      "epoch": 0.4653217493741717,
      "grad_norm": 0.3845832943916321,
      "learning_rate": 2.6740487689951704e-05,
      "loss": 0.5655944442749024,
      "step": 3950
    },
    {
      "epoch": 0.471211898100427,
      "grad_norm": 0.47510308027267456,
      "learning_rate": 2.644598892684651e-05,
      "loss": 0.5515386962890625,
      "step": 4000
    },
    {
      "epoch": 0.4771020468266824,
      "grad_norm": 0.44528424739837646,
      "learning_rate": 2.6151490163741316e-05,
      "loss": 0.5764485168457031,
      "step": 4050
    },
    {
      "epoch": 0.4829921955529377,
      "grad_norm": 0.5201848149299622,
      "learning_rate": 2.5856991400636117e-05,
      "loss": 0.5877648544311523,
      "step": 4100
    },
    {
      "epoch": 0.488882344279193,
      "grad_norm": 0.414110392332077,
      "learning_rate": 2.556249263753092e-05,
      "loss": 0.5620623397827148,
      "step": 4150
    },
    {
      "epoch": 0.4947724930054484,
      "grad_norm": 0.4384664297103882,
      "learning_rate": 2.526799387442573e-05,
      "loss": 0.5421938323974609,
      "step": 4200
    },
    {
      "epoch": 0.5006626417317037,
      "grad_norm": 0.4864569306373596,
      "learning_rate": 2.4973495111320533e-05,
      "loss": 0.5503600692749023,
      "step": 4250
    },
    {
      "epoch": 0.5065527904579591,
      "grad_norm": 0.41281092166900635,
      "learning_rate": 2.4678996348215337e-05,
      "loss": 0.5645321273803711,
      "step": 4300
    },
    {
      "epoch": 0.5124429391842144,
      "grad_norm": 0.428428053855896,
      "learning_rate": 2.4384497585110145e-05,
      "loss": 0.5577962493896484,
      "step": 4350
    },
    {
      "epoch": 0.5183330879104697,
      "grad_norm": 0.5333624482154846,
      "learning_rate": 2.408999882200495e-05,
      "loss": 0.5606404876708985,
      "step": 4400
    },
    {
      "epoch": 0.5242232366367251,
      "grad_norm": 0.40227797627449036,
      "learning_rate": 2.3795500058899753e-05,
      "loss": 0.536447868347168,
      "step": 4450
    },
    {
      "epoch": 0.5301133853629804,
      "grad_norm": 0.5653536319732666,
      "learning_rate": 2.3501001295794558e-05,
      "loss": 0.5438257217407226,
      "step": 4500
    },
    {
      "epoch": 0.5360035340892357,
      "grad_norm": 0.36576443910598755,
      "learning_rate": 2.3206502532689365e-05,
      "loss": 0.5293700408935547,
      "step": 4550
    },
    {
      "epoch": 0.5418936828154911,
      "grad_norm": 0.3761509358882904,
      "learning_rate": 2.291200376958417e-05,
      "loss": 0.5506244277954102,
      "step": 4600
    },
    {
      "epoch": 0.5477838315417465,
      "grad_norm": 0.5464377999305725,
      "learning_rate": 2.2617505006478974e-05,
      "loss": 0.5217686080932618,
      "step": 4650
    },
    {
      "epoch": 0.5536739802680017,
      "grad_norm": 0.5125371813774109,
      "learning_rate": 2.2323006243373778e-05,
      "loss": 0.5416772842407227,
      "step": 4700
    },
    {
      "epoch": 0.5595641289942571,
      "grad_norm": 0.5095928907394409,
      "learning_rate": 2.2028507480268586e-05,
      "loss": 0.5509111785888672,
      "step": 4750
    },
    {
      "epoch": 0.5654542777205125,
      "grad_norm": 0.4291091859340668,
      "learning_rate": 2.173400871716339e-05,
      "loss": 0.5686484527587891,
      "step": 4800
    },
    {
      "epoch": 0.5713444264467678,
      "grad_norm": 0.45409759879112244,
      "learning_rate": 2.1439509954058194e-05,
      "loss": 0.5536226654052734,
      "step": 4850
    },
    {
      "epoch": 0.5772345751730231,
      "grad_norm": 0.4092535376548767,
      "learning_rate": 2.1145011190953e-05,
      "loss": 0.5319575500488282,
      "step": 4900
    },
    {
      "epoch": 0.5831247238992785,
      "grad_norm": 0.40136635303497314,
      "learning_rate": 2.0850512427847803e-05,
      "loss": 0.5639181518554688,
      "step": 4950
    },
    {
      "epoch": 0.5890148726255338,
      "grad_norm": 0.5408802628517151,
      "learning_rate": 2.055601366474261e-05,
      "loss": 0.5525526428222656,
      "step": 5000
    },
    {
      "epoch": 0.5949050213517891,
      "grad_norm": 0.5099866390228271,
      "learning_rate": 2.026151490163741e-05,
      "loss": 0.5445430374145508,
      "step": 5050
    },
    {
      "epoch": 0.6007951700780445,
      "grad_norm": 0.4638907313346863,
      "learning_rate": 1.996701613853222e-05,
      "loss": 0.548079719543457,
      "step": 5100
    },
    {
      "epoch": 0.6066853188042998,
      "grad_norm": 0.39911749958992004,
      "learning_rate": 1.9672517375427023e-05,
      "loss": 0.5670225143432617,
      "step": 5150
    },
    {
      "epoch": 0.6125754675305551,
      "grad_norm": 0.5184018611907959,
      "learning_rate": 1.937801861232183e-05,
      "loss": 0.5354693222045899,
      "step": 5200
    },
    {
      "epoch": 0.6184656162568105,
      "grad_norm": 0.5053614377975464,
      "learning_rate": 1.9083519849216632e-05,
      "loss": 0.5323013305664063,
      "step": 5250
    },
    {
      "epoch": 0.6243557649830658,
      "grad_norm": 0.3327966332435608,
      "learning_rate": 1.878902108611144e-05,
      "loss": 0.5722791671752929,
      "step": 5300
    },
    {
      "epoch": 0.6302459137093211,
      "grad_norm": 0.4429830312728882,
      "learning_rate": 1.8494522323006244e-05,
      "loss": 0.5285391616821289,
      "step": 5350
    },
    {
      "epoch": 0.6361360624355765,
      "grad_norm": 0.48708465695381165,
      "learning_rate": 1.820002355990105e-05,
      "loss": 0.5447967910766601,
      "step": 5400
    },
    {
      "epoch": 0.6420262111618318,
      "grad_norm": 0.48588764667510986,
      "learning_rate": 1.7905524796795856e-05,
      "loss": 0.531166000366211,
      "step": 5450
    },
    {
      "epoch": 0.6479163598880872,
      "grad_norm": 0.5761677026748657,
      "learning_rate": 1.761102603369066e-05,
      "loss": 0.5444977188110351,
      "step": 5500
    },
    {
      "epoch": 0.6538065086143425,
      "grad_norm": 0.4203745722770691,
      "learning_rate": 1.7316527270585464e-05,
      "loss": 0.5522207260131836,
      "step": 5550
    },
    {
      "epoch": 0.6596966573405979,
      "grad_norm": 0.4315188527107239,
      "learning_rate": 1.702202850748027e-05,
      "loss": 0.5557161331176758,
      "step": 5600
    },
    {
      "epoch": 0.6655868060668532,
      "grad_norm": 0.6065393686294556,
      "learning_rate": 1.6727529744375076e-05,
      "loss": 0.5512639999389648,
      "step": 5650
    },
    {
      "epoch": 0.6714769547931085,
      "grad_norm": 0.5191645622253418,
      "learning_rate": 1.6433030981269877e-05,
      "loss": 0.5468270492553711,
      "step": 5700
    },
    {
      "epoch": 0.6773671035193639,
      "grad_norm": 0.5124340057373047,
      "learning_rate": 1.6138532218164685e-05,
      "loss": 0.5746969223022461,
      "step": 5750
    },
    {
      "epoch": 0.6832572522456192,
      "grad_norm": 0.41066256165504456,
      "learning_rate": 1.584403345505949e-05,
      "loss": 0.5498777008056641,
      "step": 5800
    },
    {
      "epoch": 0.6891474009718745,
      "grad_norm": 0.44400647282600403,
      "learning_rate": 1.5549534691954297e-05,
      "loss": 0.5308725357055664,
      "step": 5850
    },
    {
      "epoch": 0.6950375496981299,
      "grad_norm": 0.5205218195915222,
      "learning_rate": 1.52550359288491e-05,
      "loss": 0.558848762512207,
      "step": 5900
    },
    {
      "epoch": 0.7009276984243852,
      "grad_norm": 0.6072520017623901,
      "learning_rate": 1.4960537165743903e-05,
      "loss": 0.5653870773315429,
      "step": 5950
    },
    {
      "epoch": 0.7068178471506406,
      "grad_norm": 0.4234763979911804,
      "learning_rate": 1.466603840263871e-05,
      "loss": 0.561047592163086,
      "step": 6000
    },
    {
      "epoch": 0.7127079958768959,
      "grad_norm": 0.4498145878314972,
      "learning_rate": 1.4371539639533515e-05,
      "loss": 0.5746379089355469,
      "step": 6050
    },
    {
      "epoch": 0.7185981446031512,
      "grad_norm": 0.606180727481842,
      "learning_rate": 1.4077040876428318e-05,
      "loss": 0.541369400024414,
      "step": 6100
    },
    {
      "epoch": 0.7244882933294066,
      "grad_norm": 0.554951012134552,
      "learning_rate": 1.3782542113323124e-05,
      "loss": 0.5419533157348633,
      "step": 6150
    },
    {
      "epoch": 0.7303784420556619,
      "grad_norm": 0.4074244797229767,
      "learning_rate": 1.348804335021793e-05,
      "loss": 0.5530061340332031,
      "step": 6200
    },
    {
      "epoch": 0.7362685907819172,
      "grad_norm": 0.4555279314517975,
      "learning_rate": 1.3193544587112736e-05,
      "loss": 0.5379402542114258,
      "step": 6250
    },
    {
      "epoch": 0.7421587395081726,
      "grad_norm": 0.5408925414085388,
      "learning_rate": 1.2899045824007538e-05,
      "loss": 0.5367977905273438,
      "step": 6300
    },
    {
      "epoch": 0.7480488882344279,
      "grad_norm": 0.43790367245674133,
      "learning_rate": 1.2604547060902344e-05,
      "loss": 0.533871078491211,
      "step": 6350
    },
    {
      "epoch": 0.7539390369606832,
      "grad_norm": 0.44761922955513,
      "learning_rate": 1.231004829779715e-05,
      "loss": 0.5305088806152344,
      "step": 6400
    },
    {
      "epoch": 0.7598291856869386,
      "grad_norm": 0.5006183981895447,
      "learning_rate": 1.2015549534691956e-05,
      "loss": 0.5514130020141601,
      "step": 6450
    },
    {
      "epoch": 0.765719334413194,
      "grad_norm": 0.4621959328651428,
      "learning_rate": 1.172105077158676e-05,
      "loss": 0.512634162902832,
      "step": 6500
    },
    {
      "epoch": 0.7716094831394493,
      "grad_norm": 0.4045972526073456,
      "learning_rate": 1.1426552008481565e-05,
      "loss": 0.5374508666992187,
      "step": 6550
    },
    {
      "epoch": 0.7774996318657046,
      "grad_norm": 0.47089534997940063,
      "learning_rate": 1.113205324537637e-05,
      "loss": 0.5397818756103515,
      "step": 6600
    },
    {
      "epoch": 0.78338978059196,
      "grad_norm": 0.3840354382991791,
      "learning_rate": 1.0837554482271175e-05,
      "loss": 0.5139696502685547,
      "step": 6650
    },
    {
      "epoch": 0.7892799293182153,
      "grad_norm": 0.5506871938705444,
      "learning_rate": 1.054305571916598e-05,
      "loss": 0.5255638885498047,
      "step": 6700
    },
    {
      "epoch": 0.7951700780444706,
      "grad_norm": 0.6572314500808716,
      "learning_rate": 1.0248556956060785e-05,
      "loss": 0.5434469985961914,
      "step": 6750
    },
    {
      "epoch": 0.801060226770726,
      "grad_norm": 0.5532305240631104,
      "learning_rate": 9.95405819295559e-06,
      "loss": 0.5588301849365235,
      "step": 6800
    },
    {
      "epoch": 0.8069503754969813,
      "grad_norm": 0.5964723825454712,
      "learning_rate": 9.659559429850396e-06,
      "loss": 0.5577790069580079,
      "step": 6850
    },
    {
      "epoch": 0.8128405242232366,
      "grad_norm": 0.39614173769950867,
      "learning_rate": 9.3650606667452e-06,
      "loss": 0.5016941833496094,
      "step": 6900
    },
    {
      "epoch": 0.818730672949492,
      "grad_norm": 0.5730859637260437,
      "learning_rate": 9.070561903640006e-06,
      "loss": 0.5372100448608399,
      "step": 6950
    },
    {
      "epoch": 0.8246208216757474,
      "grad_norm": 0.5840622186660767,
      "learning_rate": 8.77606314053481e-06,
      "loss": 0.5929810714721679,
      "step": 7000
    },
    {
      "epoch": 0.8305109704020026,
      "grad_norm": 0.45500293374061584,
      "learning_rate": 8.481564377429616e-06,
      "loss": 0.5352310943603515,
      "step": 7050
    },
    {
      "epoch": 0.836401119128258,
      "grad_norm": 0.40024521946907043,
      "learning_rate": 8.18706561432442e-06,
      "loss": 0.5415795516967773,
      "step": 7100
    },
    {
      "epoch": 0.8422912678545134,
      "grad_norm": 0.41688022017478943,
      "learning_rate": 7.892566851219226e-06,
      "loss": 0.5498234176635742,
      "step": 7150
    },
    {
      "epoch": 0.8481814165807686,
      "grad_norm": 0.5618077516555786,
      "learning_rate": 7.59806808811403e-06,
      "loss": 0.5579325103759766,
      "step": 7200
    },
    {
      "epoch": 0.854071565307024,
      "grad_norm": 0.5124406218528748,
      "learning_rate": 7.303569325008836e-06,
      "loss": 0.5304927062988282,
      "step": 7250
    },
    {
      "epoch": 0.8599617140332794,
      "grad_norm": 0.4983423054218292,
      "learning_rate": 7.00907056190364e-06,
      "loss": 0.5413719940185547,
      "step": 7300
    },
    {
      "epoch": 0.8658518627595346,
      "grad_norm": 0.5468522906303406,
      "learning_rate": 6.714571798798446e-06,
      "loss": 0.5493537521362305,
      "step": 7350
    },
    {
      "epoch": 0.87174201148579,
      "grad_norm": 0.48862653970718384,
      "learning_rate": 6.42007303569325e-06,
      "loss": 0.5689895248413086,
      "step": 7400
    },
    {
      "epoch": 0.8776321602120454,
      "grad_norm": 0.3815353810787201,
      "learning_rate": 6.125574272588055e-06,
      "loss": 0.5298897171020508,
      "step": 7450
    },
    {
      "epoch": 0.8835223089383006,
      "grad_norm": 0.43773940205574036,
      "learning_rate": 5.83107550948286e-06,
      "loss": 0.5374289321899414,
      "step": 7500
    },
    {
      "epoch": 0.889412457664556,
      "grad_norm": 0.35146424174308777,
      "learning_rate": 5.5365767463776655e-06,
      "loss": 0.5881414794921875,
      "step": 7550
    },
    {
      "epoch": 0.8953026063908114,
      "grad_norm": 0.39305606484413147,
      "learning_rate": 5.242077983272471e-06,
      "loss": 0.5352141189575196,
      "step": 7600
    },
    {
      "epoch": 0.9011927551170668,
      "grad_norm": 0.3194645643234253,
      "learning_rate": 4.947579220167276e-06,
      "loss": 0.5144557189941407,
      "step": 7650
    },
    {
      "epoch": 0.907082903843322,
      "grad_norm": 0.567966103553772,
      "learning_rate": 4.653080457062081e-06,
      "loss": 0.5540130233764649,
      "step": 7700
    },
    {
      "epoch": 0.9129730525695774,
      "grad_norm": 0.49515393376350403,
      "learning_rate": 4.358581693956885e-06,
      "loss": 0.551633644104004,
      "step": 7750
    },
    {
      "epoch": 0.9188632012958328,
      "grad_norm": 0.6334394216537476,
      "learning_rate": 4.06408293085169e-06,
      "loss": 0.5393463134765625,
      "step": 7800
    },
    {
      "epoch": 0.924753350022088,
      "grad_norm": 0.6032251715660095,
      "learning_rate": 3.7695841677464954e-06,
      "loss": 0.5647381210327148,
      "step": 7850
    },
    {
      "epoch": 0.9306434987483434,
      "grad_norm": 0.4705750048160553,
      "learning_rate": 3.4750854046413005e-06,
      "loss": 0.5601213836669922,
      "step": 7900
    },
    {
      "epoch": 0.9365336474745988,
      "grad_norm": 0.48414862155914307,
      "learning_rate": 3.1805866415361056e-06,
      "loss": 0.5759140396118164,
      "step": 7950
    },
    {
      "epoch": 0.942423796200854,
      "grad_norm": 0.45923665165901184,
      "learning_rate": 2.8860878784309107e-06,
      "loss": 0.5375250244140625,
      "step": 8000
    }
  ],
  "logging_steps": 50,
  "max_steps": 8489,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.74835334381568e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
